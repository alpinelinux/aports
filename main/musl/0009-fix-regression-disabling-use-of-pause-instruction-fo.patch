From 5c3412d22555d03a1c00578ba8faaa8dc9206420 Mon Sep 17 00:00:00 2001
From: Rich Felker <dalias@aerifal.cx>
Date: Tue, 29 Mar 2016 21:22:52 -0400
Subject: [PATCH] fix regression disabling use of pause instruction for x86
 a_spin

commits e24984efd5c6ac5ea8e6cb6cd914fa8435d458bc and
16b55298dc4b6a54d287d7494e04542667ef8861 inadvertently disabled the
a_spin implementations for i386, x86_64, and x32 by defining a macro
named a_pause instead of a_spin. this should not have caused any
functional regression, but it inhibited cpu relaxation while spinning
for locks.

bug reported by George Kulakowski.
---
 arch/i386/atomic_arch.h   | 2 +-
 arch/x32/atomic_arch.h    | 2 +-
 arch/x86_64/atomic_arch.h | 2 +-
 3 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/arch/i386/atomic_arch.h b/arch/i386/atomic_arch.h
index 6e67c4c..2b1a049 100644
--- a/arch/i386/atomic_arch.h
+++ b/arch/i386/atomic_arch.h
@@ -71,7 +71,7 @@ static inline void a_barrier()
 	__asm__ __volatile__( "" : : : "memory" );
 }
 
-#define a_pause a_pause
+#define a_spin a_spin
 static inline void a_spin()
 {
 	__asm__ __volatile__( "pause" : : : "memory" );
diff --git a/arch/x32/atomic_arch.h b/arch/x32/atomic_arch.h
index 26098d3..7daf4ae 100644
--- a/arch/x32/atomic_arch.h
+++ b/arch/x32/atomic_arch.h
@@ -87,7 +87,7 @@ static inline void a_barrier()
 	__asm__ __volatile__( "" : : : "memory" );
 }
 
-#define a_pause a_pause
+#define a_spin a_spin
 static inline void a_spin()
 {
 	__asm__ __volatile__( "pause" : : : "memory" );
diff --git a/arch/x86_64/atomic_arch.h b/arch/x86_64/atomic_arch.h
index 9f47f80..55fc6fb 100644
--- a/arch/x86_64/atomic_arch.h
+++ b/arch/x86_64/atomic_arch.h
@@ -96,7 +96,7 @@ static inline void a_barrier()
 	__asm__ __volatile__( "" : : : "memory" );
 }
 
-#define a_pause a_pause
+#define a_spin a_spin
 static inline void a_spin()
 {
 	__asm__ __volatile__( "pause" : : : "memory" );
-- 
2.7.4

